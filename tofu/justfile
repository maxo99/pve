## Justfile for Terraform/OpenTofu Operations

set dotenv-load

ROOT_SSH := "ssh pve-01"
# TF_LOG_PATH := "./.tflog/tf-debug.log"
DATE_TIME := "$(date +%Y-%m-%d-%H%M%S)"
TF_LOG_LEVEL := "DEBUG"
TF_LOG_PATH := "./.tflog/tf-debug-" + DATE_TIME + ".log"

# DATE := env('DATE', shell('date +%F'))
#
default:
    just --list

test:
    echo "{{ TF_LOG_PATH }}"
    echo "{{ DATE_TIME }}"
#     echo "{{ datetime() }}"
#
#
#
#                                                                     
# Emergency cleanup for hanging resources
emergency-cleanup:
    #!/usr/bin/env bash
    echo "üö® Emergency cleanup of problematic OpenTofu state..."
    
    echo "1. Removing deposed objects..."
    tofu state list | grep "deposed" | while read resource; do
        echo "  Removing: $resource"
        tofu state rm "$resource" || true
    done
    
    echo "2. Force unlocking state if locked..."
    tofu force-unlock -force || true
    
    echo "3. Cleaning temporary files..."
    rm -f .terraform/terraform.tfstate*
    rm -f terraform.tfstate.backup.*.tmp
    
    echo "‚úÖ Emergency cleanup completed"

#
# Quick status check without hanging operations
status-check:
    #!/usr/bin/env bash
    echo "üìä OpenTofu Status Check (no refresh)"
    echo "State resources:"
    tofu state list | wc -l
    echo ""
    echo "Configuration validation:"
    tofu validate
    echo ""
    echo "Provider status:"
    timeout 10 tofu providers || echo "Provider check timed out"

#
# Targeted apply for specific resource types
apply-files-only:
    TF_PARALLELISM=1 tofu apply -target='proxmox_virtual_environment_file.lxc_hook_script' -target='module.lxcs.proxmox_virtual_environment_file.cloud_init_snippet'

apply-passwords-only:
    TF_PARALLELISM=1 tofu apply -target='module.lxcs.random_password.admin_password' -target='module.lxcs.null_resource.store_password_in_vault'



tf-plan-lxc name:
    tofu plan -target='module.lxcs["{{name}}"]'

tf-apply-lxc name:
    tofu apply -target='module.lxcs["{{name}}"]'

tf-destroy-lxc name:
    tofu destroy -target='module.lxcs["{{name}}"]'

# Test the official cloud-init approach after recreating VM
vm-test-official-cloudinit:
    #!/usr/bin/env bash
    echo "=== Testing Official Cloud-Init Approach ==="
    
    echo "1. Destroying current VM..."
    just vm-destroy webserver
    
    echo "2. Recreating VM with official user_data_file_id approach..."
    just vm-create webserver
    
    echo "3. Waiting for VM to be ready..."
    sleep 30
    
    echo "4. Checking cloud-init status..."
    just vm-cloudinit-check webserver
    
    echo "5. Checking if custom config is applied..."
    ssh pve-01 "qm cloudinit dump 1005 user" | head -20

show:
    tofu show

list:
    tofu state list

output:
    tofu output

#
# Prints a tab-separated list: KEY\tID\tNAME ‚Äî easy to filter with grep/awk/cut.
_get-lxcs:
    #!/bin/bash
    tofu output -json lxcs | jq -r 'to_entries[] | "\(.key)\t\(.value.id)\t\(.value.name)"'

_get-lxcs-names:
    #!/bin/bash
    tofu output -json lxcs | jq -r 'to_entries[] | .value.name'

_get-lxcs-ids:
    #!/bin/bash
    tofu output -json lxcs | jq -r 'to_entries[] | .value.id // empty'

for-each-lxc:
    #!/bin/bash
    cmd=$(cat -)
    if [ -z "$cmd" ]; then
        echo "Usage: just <recipe>  # then pipe a heredoc into 'just for-each-lxc'"
        exit 1
    fi

    json=$(tofu output -json lxcs)
    for container in $(jq -r 'keys[]' <<< "$json"); do
        id=$(jq -r ".${container}.id" <<< "$json")
        name=$(jq -r ".${container}.name" <<< "$json")
        if [ "$id" != "null" ] && [ -n "$id" ]; then
            ID="$id" NAME="$name" bash -lc "$cmd"
        fi
    done

pve-lxc-ips:
    #!/bin/bash
    echo "=== LXC Container IP Addresses ==="
    # Iterate centralized `just _get-lxcs` output (KEY<TAB>ID<TAB>NAME)
    while IFS=$'\t' read -r key id name; do
        if [ -n "$id" ] && [ "$id" != "null" ]; then
            echo "Container: $name (ID: $id)"
            ssh pve-01 "pct exec $id -- ip addr show eth0 | grep 'inet ' | head -1 | awk '{print \$2}' | cut -d'/' -f1" 2>/dev/null || echo "  Not running or no IP"
        fi
    done < <(just _get-lxcs)

lxc-test-ssh:
    #!/bin/bash
    echo "=== Testing SSH Access to Containers ==="
    ./scripts/ssh-test.sh test-lxc-batch


tf-validate:
    tofu validate
    @echo "=== Checking JSON configuration files ==="
    @find config/ -name "*.json" -exec echo "Checking {}" \; -exec jq empty {} \;

tf-fmt:
    tofu fmt -recursive

tf-init:
    tofu init -upgrade

tf-clean:
    rm -rf .terraform/
    rm -f terraform.tfstate.backup
    rm -f .terraform.lock.hcl
    @echo "Cleaned Terraform cache files"

vault-debug:
    #!/bin/bash
    VAULT_TOKEN="${VAULT_TOKEN:-$(grep VAULT_TOKEN .env 2>/dev/null | cut -d'=' -f2)}"
    VAULT_ADDR="${VAULT_ADDR:-$(grep VAULT_ADDR .env 2>/dev/null | cut -d'=' -f2)}"
    
    if [ -z "$VAULT_TOKEN" ] || [ -z "$VAULT_ADDR" ]; then
        echo "Error: VAULT_TOKEN and VAULT_ADDR must be set"
        exit 1
    fi
    
    echo "=== Available SSH Keys ==="
    curl -s -H "X-Vault-Token: $VAULT_TOKEN" "$VAULT_ADDR/v1/terraform/metadata/ssh_keys/?list=true" | jq -r '.data.keys[]'
    
    echo -e "\n=== Available API Credentials ==="
    curl -s -H "X-Vault-Token: $VAULT_TOKEN" "$VAULT_ADDR/v1/terraform/metadata/api_credentials/?list=true" | jq -r '.data.keys[]'

pve-debug-containers:
    #!/bin/bash
    echo "=== Container Status on PVE ==="
    ssh pve-01 "pct list"
    echo -e "\n=== Hook Scripts ==="
    ssh pve-01 "ls -la /var/lib/vz/snippets/*hook* 2>/dev/null || echo 'No hook scripts found'"

lxc-check-ssh-keys:
    #!/bin/bash
    echo "=== SSH Key Configuration Check ==="
    # Get vm_deployment private key from Vault
    VAULT_TOKEN="${VAULT_TOKEN:-$(grep VAULT_TOKEN .env 2>/dev/null | cut -d'=' -f2)}"
    VAULT_ADDR="${VAULT_ADDR:-$(grep VAULT_ADDR .env 2>/dev/null | cut -d'=' -f2)}"
    
    if [ -z "$VAULT_TOKEN" ] || [ -z "$VAULT_ADDR" ]; then
        echo "Error: VAULT_TOKEN and VAULT_ADDR must be set"
        exit 1
    fi
    
    # Get all SSH key fingerprints from Vault
    echo "=== Vault SSH Key Fingerprints ==="
    for key_type in vm_deployment ansible_management proxmox_host; do
        echo "üìã $key_type:"
        curl -s -H "X-Vault-Token: $VAULT_TOKEN" "$VAULT_ADDR/v1/terraform/data/ssh_keys/$key_type" | \
            jq -r '.data.data.public_key' | ssh-keygen -l -f - 2>/dev/null || echo "  ‚ùå Failed to get fingerprint"
    done
    
    echo -e "\n=== Container SSH Key Configuration ==="
    while IFS=$'\t' read -r key id name; do
        if [ -n "$id" ] && [ "$id" != "null" ]; then
            echo "üîç $name (ID: $id):"
            ssh pve-01 "pct exec $id -- bash -c 'if [ -f /root/.ssh/authorized_keys ]; then echo "  Keys found: \$(wc -l < /root/.ssh/authorized_keys)"; cat /root/.ssh/authorized_keys | grep -v "^#" | while read key; do echo "  üîë \$(echo \$key | ssh-keygen -l -f - 2>/dev/null || echo "Invalid key")"; done; else echo "  ‚ùå No authorized_keys file"; fi'" 2>/dev/null
        fi
    done < <(just _get-lxcs)

lxc-debug-ssh name:
    #!/bin/bash
    echo "=== SSH Debug for Container: {{name}} ==="
    VAULT_TOKEN="${VAULT_TOKEN:-$(grep VAULT_TOKEN .env 2>/dev/null | cut -d'=' -f2)}"
    VAULT_ADDR="${VAULT_ADDR:-$(grep VAULT_ADDR .env 2>/dev/null | cut -d'=' -f2)}"
    
    if [ -z "$VAULT_TOKEN" ] || [ -z "$VAULT_ADDR" ]; then
        echo "Error: VAULT_TOKEN and VAULT_ADDR must be set"
        exit 1
    fi
    
    # Get container details
    id=$(tofu output -json lxcs | jq -r '.["{{name}}"].id // empty')
    container_name=$(tofu output -json lxcs | jq -r '.["{{name}}"].name // empty')
    
    if [ -z "$id" ]; then
        echo "‚ùå Container '{{name}}' not found"
        exit 1
    fi
    
    echo "üîç Container: $container_name (ID: $id)"
    
    # Get IP address
    ip=$(ssh pve-01 "pct exec $id -- ip addr show eth0 | grep 'inet ' | head -1 | awk '{print \$2}' | cut -d'/' -f1" 2>/dev/null)
    echo "üåê IP Address: ${ip:-Not found}"
    
    # Check container status
    echo "üìä Container Status:"
    ssh pve-01 "pct status $id"
    
    # Check SSH service
    echo -e "\nüîß SSH Service Status:"
    ssh pve-01 "pct exec $id -- systemctl is-active ssh" 2>/dev/null || echo "  SSH service check failed"
    
    # Check authorized_keys
    echo -e "\nüîë SSH Keys Configuration:"
    ssh pve-01 "pct exec $id -- bash -c 'if [ -f /root/.ssh/authorized_keys ]; then echo \"File exists with \$(wc -l < /root/.ssh/authorized_keys) keys:\"; cat /root/.ssh/authorized_keys; else echo \"No authorized_keys file found\"; fi'" 2>/dev/null
    
    # Test SSH connection with different keys
    echo -e "\nüß™ Testing SSH Connections:"
    for key_type in vm_deployment ansible_management proxmox_host; do
        echo "Testing with $key_type key..."
        curl -s -H "X-Vault-Token: $VAULT_TOKEN" "$VAULT_ADDR/v1/terraform/data/ssh_keys/$key_type" | \
            jq -r '.data.data.private_key' > /tmp/${key_type}_key
        chmod 600 /tmp/${key_type}_key
        
        if [ -n "$ip" ]; then
            timeout 5 ssh -i /tmp/${key_type}_key -o ConnectTimeout=3 -o StrictHostKeyChecking=no root@$ip "echo '  ‚úÖ $key_type: Success'" 2>/dev/null || echo "  ‚ùå $key_type: Failed"
        else
            echo "  ‚è≠Ô∏è  $key_type: Skipped (no IP)"
        fi
        rm -f /tmp/${key_type}_key
    done


vm-debug-ssh name:
    #!/bin/bash
    echo "=== SSH Debug for VM: {{name}} ==="
    
    # Get VM details using script
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    VM_IP=$(./scripts/get-vm-details.sh get-ip "{{name}}")
    
    if [ -z "$VM_ID" ]; then
        echo "‚ùå VM '{{name}}' not found"
        exit 1
    fi
    
    echo "üîç VM: {{name}} (ID: $VM_ID)"
    echo "üåê IP Address: ${VM_IP:-Not available}"
    
    # Check VM status
    echo -e "\nüìä VM Status:"
    ./scripts/pve-ssh.sh "qm status $VM_ID"
    
    # Check QEMU Guest Agent
    echo -e "\nü§ñ QEMU Guest Agent Status:"
    ./scripts/guest-agent.sh status "$VM_ID"
    
    # Check if we can get network info from guest agent
    echo -e "\nüåê Network Info from Guest Agent:"
    ./scripts/guest-agent.sh network-info "$VM_ID"
    
    # Test SSH connection if IP is available
    if [ -n "$VM_IP" ]; then
        echo -e "\nüß™ Testing SSH Connections:"
        ./scripts/ssh-test.sh test-connection "$VM_IP"
        
        # Test cloud-init status via SSH
        echo -e "\n‚òÅÔ∏è  Cloud-init Status (via SSH):"
        ./scripts/cloud-init.sh status "$VM_IP"
    else
        echo -e "\n‚è≠Ô∏è  SSH tests skipped (no IP available)"
    fi

#
# Test SSH to VM with specific IP (when auto-detection fails)
vm-ssh-test name ip:
    #!/bin/bash
    echo "=== SSH Test for VM: {{name}} at {{ip}} ==="
    
    # Get VM details using script
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    
    if [ -z "$VM_ID" ]; then
        echo "‚ùå VM '{{name}}' not found"
        exit 1
    fi
    
    echo "üîç VM: {{name}} (ID: $VM_ID) at IP: {{ip}}"
    
    # Test SSH connection
    echo -e "\nüß™ Testing SSH Connections:"
    ./scripts/ssh-test.sh test-connection "{{ip}}"
    
    # Test cloud-init and guest agent status via SSH
    echo -e "\n‚òÅÔ∏è  Detailed Status Check (via SSH):"
    ./scripts/cloud-init.sh status "{{ip}}"

#
# Get VM IP addresses (if guest agent is working)
vm-get-ip name:
    #!/bin/bash
    echo "=== Getting IP for VM: {{name}} ==="
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    if [ -z "$VM_ID" ]; then
        echo "‚ùå VM '{{name}}' not found"
        exit 1
    fi
    ./scripts/guest-agent.sh network-info "$VM_ID" | jq -r '.[] | select(.name != "lo") | .["ip-addresses"][]? | select(.["ip-address-type"] == "ipv4") | .["ip-address"]' 2>/dev/null || echo "‚ùå Guest agent not available or no IP found"

#
# Test VM with password authentication (for debugging)
vm-password-test name ip password:
    #!/bin/bash
    echo "=== Password Auth Test for VM: {{name}} at {{ip}} ==="
    ./scripts/ssh-test.sh test-password "{{ip}}" "ubuntu" "{{password}}"
    ./scripts/ssh-test.sh test-password "{{ip}}" "root" "{{password}}"

#
# Quick cloud-init check (no waiting)
vm-cloudinit-check name:
    #!/usr/bin/env bash
    echo "=== Quick Cloud-init Check for VM: {{name}} ==="
    VM_IP=$(./scripts/get-vm-details.sh get-ip "{{name}}")
    if [ -z "$VM_IP" ]; then
        echo "‚ùå Could not get IP for VM {{name}}"
        exit 1
    fi
    echo "VM IP: $VM_IP"
    ./scripts/cloud-init.sh status "$VM_IP"

#
# Validate VM configuration for invalid characters
vm-validate-config name:
    #!/usr/bin/env bash
    echo "=== Validating VM Configuration for Invalid Characters: {{name}} ==="
    ./scripts/cloud-init.sh validate-config "{{name}}"

#
# Fix VM cloud-init configuration manually
vm-fix-cloudinit name:
    #!/usr/bin/env bash
    echo "=== Fixing Cloud-init Configuration for VM: {{name}} ==="
    ./scripts/cloud-init.sh fix-config "{{name}}"

#
# VM Management and Debugging Recipes
vm-create name:
    #!/usr/bin/env bash
    echo "=== Creating VM: {{name}} ==="
    TF_LOG="${TF_LOG_LEVEL}" TF_LOG_PATH="${TF_LOG_PATH}" tofu apply -target="module.vms[\"{{name}}\"]" -auto-approve

    if [ $? -eq 0 ]; then
        echo "‚úÖ VM creation completed successfully"
        echo ""
        echo "üîç Checking VM status and cloud-init..."
        
        # Get VM details using script
        VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
        VM_IP=$(./scripts/get-vm-details.sh get-ip "{{name}}")
        
        if [ -n "$VM_ID" ]; then
            echo "üìã VM ID: $VM_ID"
            
            # Wait a moment for VM to start
            echo "‚è±Ô∏è Waiting 10 seconds for VM to initialize..."
            sleep 10
            
            # Check VM status
            echo "üìä VM Status:"
            ./scripts/pve-ssh.sh "qm status $VM_ID"
            
            # Check guest agent (may take time to start)
            echo ""
            echo "ü§ñ Guest Agent Status:"
            ./scripts/guest-agent.sh status "$VM_ID"
            
            # Get IP address if agent is working
            if [ -n "$VM_IP" ]; then
                echo ""
                echo "üåê Network Information:"
                echo "  üìç IP Address: $VM_IP"
            else
                echo ""
                echo "üåê Network Information:"
                echo "  ‚è≥ No IP assigned yet"
            fi
            
            # Check cloud-init status
            echo ""
            echo "‚òÅÔ∏è Cloud-init Status:"
            if [ -n "$VM_IP" ]; then
                ./scripts/cloud-init.sh status "$VM_IP"
            else
                echo "  ‚è≥ Cannot check cloud-init status (no IP yet)"
            fi
            
            echo ""
            echo "üí° Next steps:"
            echo "  - Run 'just vm-cloudinit-check {{name}}' for detailed cloud-init verification"
            echo "  - Run 'just vm-debug-ssh {{name}}' to test SSH connections once ready"
            echo "  - Check guest agent status: 'just vm-status {{name}}'"
        else
            echo "‚ùå Could not get VM ID from Terraform output"
        fi
    else
        echo "‚ùå VM creation failed"
        exit 1
    fi
#
#
vm-destroy name:
    #!/usr/bin/env bash
    echo "=== Destroying VM: {{name}} ==="
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    if [ -n "$VM_ID" ]; then
        echo "üõë Stopping VM $VM_ID via Proxmox..."
        ./scripts/pve-ssh.sh "qm stop $VM_ID" || true
        echo "üóëÔ∏è Destroying VM $VM_ID via Proxmox..."
        ./scripts/pve-ssh.sh "qm destroy $VM_ID --purge" || true
    fi
    echo "üßπ Removing from Terraform state..."
    tofu destroy -target="module.vms[\"{{name}}\"]" -auto-approve

#
# Complete cleanup for VM - handles prevent_destroy lifecycle rules
vm-cleanup name:
    #!/usr/bin/env bash
    echo "=== Complete VM Cleanup: {{name}} ==="
    
    # Get VM ID first
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    
    if [ -n "$VM_ID" ]; then
        echo "üõë Stopping VM $VM_ID via Proxmox..."
        ./scripts/pve-ssh.sh "qm stop $VM_ID" || true
        echo "üóëÔ∏è Destroying VM $VM_ID via Proxmox..."
        ./scripts/pve-ssh.sh "qm destroy $VM_ID --purge" || true
    fi
    
    echo "üßπ Removing VM resource from Terraform state..."
    tofu destroy -target="module.vms[\"{{name}}\"].proxmox_virtual_environment_vm.vm" -auto-approve || true
    
    echo "üóÇÔ∏è Removing user data file from Terraform state..."
    tofu destroy -target="module.vms[\"{{name}}\"].proxmox_virtual_environment_file.user_data" -auto-approve || true
    
    echo "üîë Removing password resource (bypassing prevent_destroy)..."
    tofu state rm "module.vms[\"{{name}}\"].random_password.admin_password[0]" 2>/dev/null || true
    tofu state rm "module.vms[\"{{name}}\"].null_resource.store_password_in_vault[0]" 2>/dev/null || true
    
    echo "‚úÖ Complete cleanup finished for {{name}}"

#
# Recreate VM with automatic cleanup and creation
vm-recreate name:
    #!/usr/bin/env bash
    echo "=== Recreating VM: {{name}} with cleanup ==="
    
    echo "üßπ Step 1: Complete cleanup..."
    just vm-cleanup {{name}}
    
    echo "‚è±Ô∏è Step 2: Waiting 5 seconds for cleanup to settle..."
    sleep 5
    
    echo "üöÄ Step 3: Creating VM with enhanced status checking..."
    just vm-create {{name}}

vm-status name: 
    #!/usr/bin/env bash
    echo "=== VM Status: {{name}} ==="
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    if [ -z "$VM_ID" ]; then
        echo "‚ùå VM not found in Terraform state"
        exit 1
    fi
    
    echo "üîç VM: {{name}} (ID: $VM_ID)"
    echo "üìã Proxmox VM Status:"
    ./scripts/pve-ssh.sh "qm status $VM_ID" || echo "‚ùå VM not found in Proxmox"
    
    echo -e "\nüåê Network Information:"
    ./scripts/guest-agent.sh network-info "$VM_ID"

vm-cloud-init-status name:
    #!/usr/bin/env bash
    echo "=== Cloud-init Status for VM: {{name}} ==="
    VM_IP=$(./scripts/get-vm-details.sh get-ip "{{name}}")
    if [ -z "$VM_IP" ]; then
        echo "‚ùå Could not get IP for VM {{name}}"
        exit 1
    fi
    ./scripts/cloud-init.sh status "$VM_IP"

vm-cloud-init-full name:
    #!/usr/bin/env bash
    echo "=== Full Cloud-init Configuration for VM: {{name}} ==="
    ./scripts/cloud-init.sh show-config "{{name}}"

vm-cloud-init-logs name:
    #!/usr/bin/env bash
    echo "=== Comprehensive Cloud-init Logs for VM: {{name}} ==="
    VM_IP=$(./scripts/get-vm-details.sh get-ip "{{name}}")
    if [ -z "$VM_IP" ]; then
        echo "‚ùå Could not get IP for VM {{name}}"
        exit 1
    fi
    ./scripts/cloud-init.sh logs "$VM_IP"

# Monitor cloud-init operations in real-time
vm-monitor-cloudinit name:
    #!/usr/bin/env bash
    echo "=== Real-time Cloud-init Monitoring for VM: {{name}} ==="
    ./scripts/cloud-init.sh monitor "{{name}}"

# View cloud-init merge operation logs - where custom config is applied over Proxmox defaults
vm-cloud-init-merge-logs name:
    #!/usr/bin/env bash
    echo "=== Cloud-init Merge Operation Logs for VM: {{name}} ==="
    VM_IP=$(./scripts/get-vm-details.sh get-ip "{{name}}")
    if [ -z "$VM_IP" ]; then
        echo "‚ùå Could not get IP for VM {{name}}"
        exit 1
    fi
    ./scripts/cloud-init.sh merge-logs "$VM_IP"

vm-guest-agent-debug name:
    #!/usr/bin/env bash
    echo "=== Guest Agent Debug for VM: {{name}} ==="
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    if [ -z "$VM_ID" ]; then
        echo "‚ùå VM not found"
        exit 1
    fi
    ./scripts/guest-agent.sh debug "$VM_ID"

#
## Cancel current apply and recreate VM with fixed config
vm-recreate-with-fix name:
    #!/bin/bash
    echo "=== Recreating VM {{name}} with fixed configuration ==="
    
    # Get VM ID
    VM_ID=$(./scripts/get-vm-details.sh "{{name}}" id)
    if [ -z "$VM_ID" ]; then
        echo "‚ùå VM '{{name}}' not found"
        exit 1
    fi
    
    echo "üõë Stopping any running apply operations..."
    pkill -f "tofu apply" || true
    
    echo "üóëÔ∏è  Destroying current VM..."
    tofu destroy -target='module.vms["{{name}}"]' -auto-approve
    
    echo "üîß Applying with fixed cloud-init configuration..."
    tofu apply -target='module.vms["{{name}}"]' -auto-approve



tf-emergency-cleanup:
    #!/bin/bash
    echo "‚ö†Ô∏è  This will destroy ALL infrastructure and clean cache files!"
    read -p "Are you sure? (yes/no): " confirm
    if [ "$confirm" = "yes" ]; then
        just destroy-auto
        just clean
        echo "Emergency cleanup completed"
    else
        echo "Cancelled"
    fi
#
#
#
#
##
#
tf-debug *args:
    TF_LOG={{ TF_LOG_LEVEL}} TF_LOG_PATH={{ TF_LOG_PATH }} tofu {{args}}
#     TF_LOG=DEBUG TF_LOG_PATH="{{ TF_LOG_PATH }}" tofu {{args}}


tfad:
    just tf-debug apply --auto-approve


tfdd:
    TF_LOG=DEBUG TF_LOG_PATH="./.tflog/tf-debug.log" tofu destroy --auto-approve


tf-plan-debug:
    just tf-debug plan



tf-pve-vm-check-guest-agent vm_id:
    @echo "Checking QEMU Guest Agent status..."
    ssh pve-01 "qm agent {{vm_id}} ping" || echo "Guest agent not responding"
    ssh pve-01 "qm config {{vm_id}} | grep agent" || echo "No agent config found"

tf-pve-vm-check-status vm_id:
    ssh pve-01 "qm status {{vm_id}}"
    ssh pve-01 "qm config {{vm_id}}"




# generate-simple-password:
#   openssl rand -base64 32 | tr -d "=+/" | cut -c1-16
# Fix SSH keys for a specific container (emergency repair)
# fix-ssh name:
#     #!/bin/bash
#     echo "=== SSH Key Emergency Fix for: {{name}} ==="
#     VAULT_TOKEN="${VAULT_TOKEN:-$(grep VAULT_TOKEN .env 2>/dev/null | cut -d'=' -f2)}"
#     VAULT_ADDR="${VAULT_ADDR:-$(grep VAULT_ADDR .env 2>/dev/null | cut -d'=' -f2)}"
    
#     if [ -z "$VAULT_TOKEN" ] || [ -z "$VAULT_ADDR" ]; then
#         echo "Error: VAULT_TOKEN and VAULT_ADDR must be set"
#         exit 1
#     fi
    
#     # Get container details
#     id=$(tofu output -json lxcs | jq -r '.["{{name}}"].id // empty')
#     if [ -z "$id" ]; then
#         echo "‚ùå Container '{{name}}' not found"
#         exit 1
#     fi
    
#     echo "üîß Fixing SSH keys for container ID: $id"
    
#     # Backup existing keys
#     echo "üíæ Backing up existing authorized_keys..."
#     ssh pve-01 "pct exec $id -- cp /root/.ssh/authorized_keys /root/.ssh/authorized_keys.backup 2>/dev/null || echo 'No existing keys to backup'"
    
#     # Get all SSH keys from Vault and reinstall them
#     echo "üîë Reinstalling SSH keys from Vault..."
#     rm -f /tmp/all_ssh_keys
#     for key_type in vm_deployment ansible_management proxmox_host; do
#         echo "Adding $key_type key..."
#         curl -s -H "X-Vault-Token: $VAULT_TOKEN" "$VAULT_ADDR/v1/terraform/data/ssh_keys/$key_type" | \
#             jq -r '.data.data.public_key' >> /tmp/all_ssh_keys
#     done
    
#     # Install keys to container
#     ssh pve-01 "pct exec $id -- mkdir -p /root/.ssh && pct exec $id -- chmod 700 /root/.ssh"
#     scp /tmp/all_ssh_keys pve-01:/tmp/new_keys
#     ssh pve-01 "cat /tmp/new_keys | pct exec $id -- tee /root/.ssh/authorized_keys > /dev/null && pct exec $id -- chmod 600 /root/.ssh/authorized_keys && rm /tmp/new_keys"
#     rm -f /tmp/all_ssh_keys
    
#     echo "‚úÖ SSH keys reinstalled. Testing connection..."
#     just debug-ssh {{name}}
#

pve-logs-plain:
    ssh pve-01 'sudo tail -n 200 -F /var/log/syslog /var/log/pve/tasks/*'

pve-logs-sys:
    ssh pve-01 'sudo journalctl -f -u pveproxy -u pvedaemon'

pve-logs-multi:
    ssh pve-01 'sudo tail -F /var/log/syslog /var/log/pve/tasks/*' | multitail -

# vzdump:
#     ssh pve-01 'sudo vzdump --all --mode snapshot'